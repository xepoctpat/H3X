version: '3.8'

services:
  # LM Studio Server Container - Production Configuration
  h3x-lmstudio:
    image: ghcr.io/lm-studio/lm-studio:latest
    container_name: h3x-lmstudio-prod
    ports:
      - "1234:1234"
    environment:
      - LMSTUDIO_API_PORT=1234
      - LMSTUDIO_MODEL_PATH=/models
      - LMSTUDIO_CORS_ALLOW_ORIGIN=https://yourdomain.com
      - LOG_LEVEL=warn
    volumes:
      - ./models:/models:ro  # Read-only for production
      - lmstudio-cache-prod:/root/.cache/lm-studio
    networks:
      - h3x-neural-network-prod
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:1234/v1/models"]
      interval: 60s  # Less frequent checks for production
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # H3X Main Application - Production Configuration
  h3x-main:
    build:
      context: .
      dockerfile: Dockerfile.h3x
      target: production
    container_name: h3x-main-prod
    ports:
      - "3978:3978"
    environment:
      - NODE_ENV=production
      - LMSTUDIO_URL=http://h3x-lmstudio:1234
      - DOCKER_MODE=true
      - H3X_NETWORK_MODE=docker
      - LOG_LEVEL=info
    volumes:
      - h3x-logs-prod:/app/logs
    depends_on:
      h3x-lmstudio:
        condition: service_healthy
    networks:
      - h3x-neural-network-prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # H3X Protocol Server - Production Configuration
  h3x-protocol:
    build:
      context: .
      dockerfile: Dockerfile.protocol
      target: production
    container_name: h3x-protocol-prod
    ports:
      - "8080:8080"
    environment:
      - LMSTUDIO_URL=http://h3x-lmstudio:1234
      - H3X_MAIN_URL=http://h3x-main:3978
      - LOG_LEVEL=info
      - GO_ENV=production
    networks:
      - h3x-neural-network-prod
    depends_on:
      - h3x-main
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # Response Output Processor - Production Configuration
  h3x-response-processor:
    build:
      context: ./Scripts
      dockerfile: Dockerfile.response-processor
      target: production
    container_name: h3x-response-processor-prod
    environment:
      - LMSTUDIO_URL=http://h3x-lmstudio:1234
      - REDIS_URL=redis://h3x-redis:6379
      - LOG_LEVEL=info
    volumes:
      - response-outputs-prod:/app/outputs
    networks:
      - h3x-neural-network-prod
    depends_on:
      - h3x-lmstudio
      - h3x-redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Redis for response caching - Production Configuration
  h3x-redis:
    image: redis:7-alpine
    container_name: h3x-redis-prod
    volumes:
      - redis-data-prod:/data
    networks:
      - h3x-neural-network-prod
    restart: unless-stopped
    command: ["redis-server", "--appendonly", "yes", "--maxmemory", "512mb", "--maxmemory-policy", "allkeys-lru"]
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Hexperiment MCP Protocol Server - Production Configuration
  hexperiment-mcp:
    build:
      context: ./hexperiment-system-protocol
      dockerfile: Dockerfile
      target: production
    container_name: hexperiment-mcp-prod
    ports:
      - "8082:8080"
    environment:
      - PORT=8080
      - GO_ENV=production
      - LOG_LEVEL=info
      - API_KEY=${MCP_API_KEY:-production-api-key}
      - CORS_ALLOWED_ORIGINS=${CORS_ORIGINS:-https://yourdomain.com}
    networks:
      - h3x-neural-network-prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # Reverse Proxy (Production only)
  h3x-proxy:
    image: nginx:alpine
    container_name: h3x-proxy-prod
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    networks:
      - h3x-neural-network-prod
    depends_on:
      - h3x-main
      - h3x-protocol
      - hexperiment-mcp
    restart: unless-stopped
    profiles:
      - proxy

  # Monitoring (Production only)
  h3x-monitor:
    image: prom/prometheus:latest
    container_name: h3x-monitor-prod
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - h3x-neural-network-prod
    restart: unless-stopped
    profiles:
      - monitoring

networks:
  h3x-neural-network-prod:
    driver: bridge
    name: h3x-neural-network-prod

volumes:
  lmstudio-cache-prod:
    name: h3x-lmstudio-cache-prod
  h3x-logs-prod:
    name: h3x-logs-prod
  response-outputs-prod:
    name: h3x-response-outputs-prod
  redis-data-prod:
    name: h3x-redis-data-prod
  prometheus-data:
    name: h3x-prometheus-data
